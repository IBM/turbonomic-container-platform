# Monitors either vLLM or TGI Servers.
# Depending on your configuration, you might have to modify labels if you have customized them
# from the defaults.
apiVersion: metrics.turbonomic.io/v1alpha1
kind: PrometheusQueryMapping
metadata:
  name: mixed-vllm-tgi
  labels:
    mapping: mixed-vllm-tgi
spec:
  entities:
  - type: application
    attributes:
    - label: container
      name: container
    - isIdentifier: true
      label: instance
      matches: \d{1,3}(?:\.\d{1,3}){3}(?::\d{1,5})??
      name: ip
    - label: namespace
      name: namespace
    - label: pod
      name: pod
    # If service is not available please use the below for service.
    # This tries to guess the service by looking at the pod name.
    # If your pod does not follow this convention, then this method
    # will not work.
    # This regex uses 2 sub regex patterns to catch service name
    # 1.(.*)-[a-f0-9]{10}-[a-z0-9]{5}$
    #   This will match when deployment/service name <47 chars where 
    #   kubernetes doesn't do any name shortening before naming pods. 
    #   [a-f0-9]{10} checks for replicaset hash, which is 10 chars
    #   when there's no shortening.
    #   [a-z0-9]{5}$ checks for pod hash which is always present at the
    #   last 5 chars.
    # 2.(.*)-[a-f0-9]{0,10}[a-z0-9]{5}$
    #   If deployment/service name >= 47 and <=57 chars, kubernetes 
    #   truncates hyphens, replicaset hash and then deployment, so as to adhere 
    #   to 63 character limit for pod.
    #   [a-f0-9]{0,10}[a-z0-9]{5}$ checks for merged replicaset hash, pod hash
    #   when there's no hyphens present between both hashes.
    # 1 and 2 both won't work if deployment is > 57 chars.
    #- label: pod
    #  as: service
    #  name: service
    #  matches: (.*)-[a-f0-9]{10}-[a-z0-9]{5}$|(.*)-[a-f0-9]{0,10}[a-z0-9]{5}$
    - label: service
      name: service
    # If service is not available please use the below for service.
    # This tries to guess the service by looking at the pod name.
    # If your pod does not follow this convention, then this method
    # will not work.
    # This regex uses 2 sub regex patterns to catch service name
    # 1.(.*)-[a-f0-9]{10}-[a-z0-9]{5}$
    #   This will match when deployment/service name <47 chars where 
    #   kubernetes doesn't do any name shortening before naming pods. 
    #   [a-f0-9]{10} checks for replicaset hash, which is 10 chars
    #   when there's no shortening.
    #   [a-z0-9]{5}$ checks for pod hash which is always present at the
    #   last 5 chars.
    # 2.(.*)-[a-f0-9]{0,10}[a-z0-9]{5}$
    #   If deployment/service name >= 47 and <=57 chars, kubernetes 
    #   truncates hyphens, replicaset hash and then deployment, so as to adhere 
    #   to 63 character limit for pod.
    #   [a-f0-9]{0,10}[a-z0-9]{5}$ checks for merged replicaset hash, pod hash
    #   when there's no hyphens present between both hashes.
    # 1 and 2 both won't work if deployment is > 57 chars.     
    #- label: pod
    #  as: service_name
    #  name: service_name
    #  matches: (.*)-[a-f0-9]{10}-[a-z0-9]{5}$|(.*)-[a-f0-9]{0,10}[a-z0-9]{5}$
    - label: service
      name: service_name
    - label: namespace
      name: service_ns
    metrics:
    - type: transaction
      queries:
      - type: used
        # The following transaction query supports both vLLM and TGI metrics.
        #
        # The vLLM portion exhibits the "X + Y OR Z == 0" pattern.  "X + Y" is the total number of tokens, while "Z" is
        # the number of successful requests which is used to ensures proper handling of the zero-transaction scenario
        # in which case we'd want this query to return 0.  However, the "token count" metric will not be 0; instead, it
        # will be unavailable as there are no requests to count the tokens.  To address this, we append the expression
        # with the "OR Z == 0" tail, which will return 0 because the request count is 0.
        # Note: it is theoretically impossible that the request count is non-zero and the token count is unavailable.
        #
        # The TGI portion exhibits the "A OR B + C OR D == 0" pattern.  A (tgi_request_total_tokens_sum) is only
        # available in the IBM variant of the TGI implementation, and not in the original HuggingFace one in which only
        # B (tgi_request_input_length_sum) and C (tgi_request_generated_tokens_sum) exist.  Their sum is A.
        promql: rate(vllm:prompt_tokens_total{}[10m]) + rate(vllm:generation_tokens_total{}[10m])
          OR rate(vllm:request_success_total{}[30m]) == 0
          OR rate(tgi_request_total_tokens_sum{}[10m])
          OR rate(tgi_request_input_length_sum{}[10m]) + rate(tgi_request_generated_tokens_sum{}[10m])
          OR rate(tgi_request_count{}[30m]) == 0
    - type: queuingTime
      queries:
      - type: used
        # The 99th-percentile of queuing time in milliseconds over the past 30 minutes, supporting both vLLM and TGI
        # metrics as long as one of them is available.
        # If there are zero observations over the 30 minutes, then the value 0 will be returned.
        promql: 1000 * (sum by (container, instance, namespace, pod, model_name) (rate(vllm:request_success_total{}[30m]) == 0)
          OR (histogram_quantile(0.99, sum by (le, container, instance, namespace, pod, model_name) (rate(vllm:request_queue_time_seconds_bucket{}[30m]))))
          OR sum by (container, instance, namespace, pod, model_name) (rate(tgi_request_count{}[30m]) == 0)
          OR (histogram_quantile(0.99, sum by (le, container, instance, namespace, pod, model_name) (rate(tgi_request_queue_duration_bucket{}[30m])))))
    - type: responseTime
      queries:
      - type: used
        # The 99th-percentile of response time in milliseconds over the past 30 minutes, supporting both vLLM and TGI
        # metrics as long as one of them is available.
        # This query will return nothing (missing data) if there are zero observations, which we think is the
        # correct behavior because without any requests recorded we can't really measure the response time which is
        # certainly not zero.
        promql: 1000 * (histogram_quantile(0.99, rate(vllm:e2e_request_latency_seconds_bucket{}[30m]))
          OR histogram_quantile(0.99, rate(tgi_request_duration_bucket{}[30m])))
    - type: serviceTime
      queries:
      - type: used
        # The 99th-percentile of "service"" time in milliseconds over the past 30 minutes.  "serviceTime" measures the
        # TPOT (time-per-output-token).  The query below covers two favors of metrics: vLLM and TGI.
        # This query will return nothing (missing data) when there are zero observations during the period, which we
        # think is the correct behavior because without any requests recorded we can't really measure the service time
        # which is certainly not zero.
        # A note on the TGI "method" filter below.  There are two types when coming to measuring inference duration:
        # - One is "prefill" which measures the time to first (output) token (TTFT).  This value varies according to the
        #   input token length.
        # - The other is called "decode" or "next_token", which measures the TPOT that is relatively stable per model.
        #   That means we can set a meaningful SLO for this metric per model and use it to drive the scaling.
        # We use a negative filter below {method != "prefill"}, instead of {method = "next_token"}, to make the query
        # work for both variants of TGI.  The HuggingFace variant uses the term "decode", while the IBM variant uses
        # "next_token".  We could also use regex pattern: {{method =~ "next_token|decode"}}.
        promql: 1000 * (histogram_quantile(0.99, rate(vllm:time_per_output_token_seconds_bucket{}[30m]))
          OR histogram_quantile(0.99, rate(tgi_batch_inference_duration_bucket{method != "prefill"}[30m])))
    - type: concurrentQueries
      queries:
      - type: used
        promql: avg_over_time(vllm:num_requests_running{}[10m]) OR avg_over_time(tgi_batch_current_size{}[10m])
          OR rate(vllm:request_success_total{}[30m]) == 0 OR rate(tgi_request_input_count{}[30m]) == 0
    - type: llmCache
      queries:
      - type: used
        # This only exists in vLLM not TGI.
        promql: avg_over_time(vllm:gpu_cache_usage_perc{}[10m])
